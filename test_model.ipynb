{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from models.model import get_model\n",
    "\n",
    "from torch import nn\n",
    "import torchinfo\n",
    "from torchprofile import profile_macs\n",
    "\n",
    "import timm\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bat_resnext26ts.ch_in1k',\n",
       " 'beit_base_patch16_224.in22k_ft_in22k',\n",
       " 'beit_base_patch16_224.in22k_ft_in22k_in1k',\n",
       " 'beit_base_patch16_384.in22k_ft_in22k_in1k',\n",
       " 'beit_large_patch16_224.in22k_ft_in22k',\n",
       " 'beit_large_patch16_224.in22k_ft_in22k_in1k',\n",
       " 'beit_large_patch16_384.in22k_ft_in22k_in1k',\n",
       " 'beit_large_patch16_512.in22k_ft_in22k_in1k',\n",
       " 'beitv2_base_patch16_224.in1k_ft_in1k',\n",
       " 'beitv2_base_patch16_224.in1k_ft_in22k',\n",
       " 'beitv2_base_patch16_224.in1k_ft_in22k_in1k',\n",
       " 'beitv2_large_patch16_224.in1k_ft_in1k',\n",
       " 'beitv2_large_patch16_224.in1k_ft_in22k',\n",
       " 'beitv2_large_patch16_224.in1k_ft_in22k_in1k',\n",
       " 'botnet26t_256.c1_in1k',\n",
       " 'caformer_b36.sail_in1k',\n",
       " 'caformer_b36.sail_in1k_384',\n",
       " 'caformer_b36.sail_in22k',\n",
       " 'caformer_b36.sail_in22k_ft_in1k',\n",
       " 'caformer_b36.sail_in22k_ft_in1k_384',\n",
       " 'caformer_m36.sail_in1k',\n",
       " 'caformer_m36.sail_in1k_384',\n",
       " 'caformer_m36.sail_in22k',\n",
       " 'caformer_m36.sail_in22k_ft_in1k',\n",
       " 'caformer_m36.sail_in22k_ft_in1k_384',\n",
       " 'caformer_s18.sail_in1k',\n",
       " 'caformer_s18.sail_in1k_384',\n",
       " 'caformer_s18.sail_in22k',\n",
       " 'caformer_s18.sail_in22k_ft_in1k',\n",
       " 'caformer_s18.sail_in22k_ft_in1k_384',\n",
       " 'caformer_s36.sail_in1k',\n",
       " 'caformer_s36.sail_in1k_384',\n",
       " 'caformer_s36.sail_in22k',\n",
       " 'caformer_s36.sail_in22k_ft_in1k',\n",
       " 'caformer_s36.sail_in22k_ft_in1k_384',\n",
       " 'cait_m36_384.fb_dist_in1k',\n",
       " 'cait_m48_448.fb_dist_in1k',\n",
       " 'cait_s24_224.fb_dist_in1k',\n",
       " 'cait_s24_384.fb_dist_in1k',\n",
       " 'cait_s36_384.fb_dist_in1k',\n",
       " 'cait_xs24_384.fb_dist_in1k',\n",
       " 'cait_xxs24_224.fb_dist_in1k',\n",
       " 'cait_xxs24_384.fb_dist_in1k',\n",
       " 'cait_xxs36_224.fb_dist_in1k',\n",
       " 'cait_xxs36_384.fb_dist_in1k',\n",
       " 'coat_lite_medium.in1k',\n",
       " 'coat_lite_medium_384.in1k',\n",
       " 'coat_lite_mini.in1k',\n",
       " 'coat_lite_small.in1k',\n",
       " 'coat_lite_tiny.in1k',\n",
       " 'coat_mini.in1k',\n",
       " 'coat_small.in1k',\n",
       " 'coat_tiny.in1k',\n",
       " 'coatnet_0_rw_224.sw_in1k',\n",
       " 'coatnet_1_rw_224.sw_in1k',\n",
       " 'coatnet_2_rw_224.sw_in12k',\n",
       " 'coatnet_2_rw_224.sw_in12k_ft_in1k',\n",
       " 'coatnet_3_rw_224.sw_in12k',\n",
       " 'coatnet_bn_0_rw_224.sw_in1k',\n",
       " 'coatnet_nano_rw_224.sw_in1k',\n",
       " 'coatnet_rmlp_1_rw2_224.sw_in12k',\n",
       " 'coatnet_rmlp_1_rw2_224.sw_in12k_ft_in1k',\n",
       " 'coatnet_rmlp_1_rw_224.sw_in1k',\n",
       " 'coatnet_rmlp_2_rw_224.sw_in1k',\n",
       " 'coatnet_rmlp_2_rw_224.sw_in12k',\n",
       " 'coatnet_rmlp_2_rw_224.sw_in12k_ft_in1k',\n",
       " 'coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k',\n",
       " 'coatnet_rmlp_nano_rw_224.sw_in1k',\n",
       " 'coatnext_nano_rw_224.sw_in1k',\n",
       " 'convformer_b36.sail_in1k',\n",
       " 'convformer_b36.sail_in1k_384',\n",
       " 'convformer_b36.sail_in22k',\n",
       " 'convformer_b36.sail_in22k_ft_in1k',\n",
       " 'convformer_b36.sail_in22k_ft_in1k_384',\n",
       " 'convformer_m36.sail_in1k',\n",
       " 'convformer_m36.sail_in1k_384',\n",
       " 'convformer_m36.sail_in22k',\n",
       " 'convformer_m36.sail_in22k_ft_in1k',\n",
       " 'convformer_m36.sail_in22k_ft_in1k_384',\n",
       " 'convformer_s18.sail_in1k',\n",
       " 'convformer_s18.sail_in1k_384',\n",
       " 'convformer_s18.sail_in22k',\n",
       " 'convformer_s18.sail_in22k_ft_in1k',\n",
       " 'convformer_s18.sail_in22k_ft_in1k_384',\n",
       " 'convformer_s36.sail_in1k',\n",
       " 'convformer_s36.sail_in1k_384',\n",
       " 'convformer_s36.sail_in22k',\n",
       " 'convformer_s36.sail_in22k_ft_in1k',\n",
       " 'convformer_s36.sail_in22k_ft_in1k_384',\n",
       " 'convit_base.fb_in1k',\n",
       " 'convit_small.fb_in1k',\n",
       " 'convit_tiny.fb_in1k',\n",
       " 'convmixer_768_32.in1k',\n",
       " 'convmixer_1024_20_ks9_p14.in1k',\n",
       " 'convmixer_1536_20.in1k',\n",
       " 'convnext_atto.d2_in1k',\n",
       " 'convnext_atto_ols.a2_in1k',\n",
       " 'convnext_base.clip_laion2b',\n",
       " 'convnext_base.clip_laion2b_augreg',\n",
       " 'convnext_base.clip_laion2b_augreg_ft_in1k',\n",
       " 'convnext_base.clip_laion2b_augreg_ft_in12k',\n",
       " 'convnext_base.clip_laion2b_augreg_ft_in12k_in1k',\n",
       " 'convnext_base.clip_laion2b_augreg_ft_in12k_in1k_384',\n",
       " 'convnext_base.clip_laiona',\n",
       " 'convnext_base.clip_laiona_320',\n",
       " 'convnext_base.clip_laiona_augreg_320',\n",
       " 'convnext_base.clip_laiona_augreg_ft_in1k_384',\n",
       " 'convnext_base.fb_in1k',\n",
       " 'convnext_base.fb_in22k',\n",
       " 'convnext_base.fb_in22k_ft_in1k',\n",
       " 'convnext_base.fb_in22k_ft_in1k_384',\n",
       " 'convnext_femto.d1_in1k',\n",
       " 'convnext_femto_ols.d1_in1k',\n",
       " 'convnext_large.fb_in1k',\n",
       " 'convnext_large.fb_in22k',\n",
       " 'convnext_large.fb_in22k_ft_in1k',\n",
       " 'convnext_large.fb_in22k_ft_in1k_384',\n",
       " 'convnext_large_mlp.clip_laion2b_augreg',\n",
       " 'convnext_large_mlp.clip_laion2b_augreg_ft_in1k',\n",
       " 'convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384',\n",
       " 'convnext_large_mlp.clip_laion2b_augreg_ft_in12k_384',\n",
       " 'convnext_large_mlp.clip_laion2b_ft_320',\n",
       " 'convnext_large_mlp.clip_laion2b_ft_soup_320',\n",
       " 'convnext_large_mlp.clip_laion2b_soup_ft_in12k_320',\n",
       " 'convnext_large_mlp.clip_laion2b_soup_ft_in12k_384',\n",
       " 'convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_320',\n",
       " 'convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_384',\n",
       " 'convnext_nano.d1h_in1k',\n",
       " 'convnext_nano.in12k',\n",
       " 'convnext_nano.in12k_ft_in1k',\n",
       " 'convnext_nano_ols.d1h_in1k',\n",
       " 'convnext_pico.d1_in1k',\n",
       " 'convnext_pico_ols.d1_in1k',\n",
       " 'convnext_small.fb_in1k',\n",
       " 'convnext_small.fb_in22k',\n",
       " 'convnext_small.fb_in22k_ft_in1k',\n",
       " 'convnext_small.fb_in22k_ft_in1k_384',\n",
       " 'convnext_small.in12k',\n",
       " 'convnext_small.in12k_ft_in1k',\n",
       " 'convnext_small.in12k_ft_in1k_384',\n",
       " 'convnext_tiny.fb_in1k',\n",
       " 'convnext_tiny.fb_in22k',\n",
       " 'convnext_tiny.fb_in22k_ft_in1k',\n",
       " 'convnext_tiny.fb_in22k_ft_in1k_384',\n",
       " 'convnext_tiny.in12k',\n",
       " 'convnext_tiny.in12k_ft_in1k',\n",
       " 'convnext_tiny.in12k_ft_in1k_384',\n",
       " 'convnext_tiny_hnf.a2h_in1k',\n",
       " 'convnext_xlarge.fb_in22k',\n",
       " 'convnext_xlarge.fb_in22k_ft_in1k',\n",
       " 'convnext_xlarge.fb_in22k_ft_in1k_384',\n",
       " 'convnext_xxlarge.clip_laion2b_rewind',\n",
       " 'convnext_xxlarge.clip_laion2b_soup',\n",
       " 'convnext_xxlarge.clip_laion2b_soup_ft_in1k',\n",
       " 'convnext_xxlarge.clip_laion2b_soup_ft_in12k',\n",
       " 'convnextv2_atto.fcmae',\n",
       " 'convnextv2_atto.fcmae_ft_in1k',\n",
       " 'convnextv2_base.fcmae',\n",
       " 'convnextv2_base.fcmae_ft_in1k',\n",
       " 'convnextv2_base.fcmae_ft_in22k_in1k',\n",
       " 'convnextv2_base.fcmae_ft_in22k_in1k_384',\n",
       " 'convnextv2_femto.fcmae',\n",
       " 'convnextv2_femto.fcmae_ft_in1k',\n",
       " 'convnextv2_huge.fcmae',\n",
       " 'convnextv2_huge.fcmae_ft_in1k',\n",
       " 'convnextv2_huge.fcmae_ft_in22k_in1k_384',\n",
       " 'convnextv2_huge.fcmae_ft_in22k_in1k_512',\n",
       " 'convnextv2_large.fcmae',\n",
       " 'convnextv2_large.fcmae_ft_in1k',\n",
       " 'convnextv2_large.fcmae_ft_in22k_in1k',\n",
       " 'convnextv2_large.fcmae_ft_in22k_in1k_384',\n",
       " 'convnextv2_nano.fcmae',\n",
       " 'convnextv2_nano.fcmae_ft_in1k',\n",
       " 'convnextv2_nano.fcmae_ft_in22k_in1k',\n",
       " 'convnextv2_nano.fcmae_ft_in22k_in1k_384',\n",
       " 'convnextv2_pico.fcmae',\n",
       " 'convnextv2_pico.fcmae_ft_in1k',\n",
       " 'convnextv2_tiny.fcmae',\n",
       " 'convnextv2_tiny.fcmae_ft_in1k',\n",
       " 'convnextv2_tiny.fcmae_ft_in22k_in1k',\n",
       " 'convnextv2_tiny.fcmae_ft_in22k_in1k_384',\n",
       " 'crossvit_9_240.in1k',\n",
       " 'crossvit_9_dagger_240.in1k',\n",
       " 'crossvit_15_240.in1k',\n",
       " 'crossvit_15_dagger_240.in1k',\n",
       " 'crossvit_15_dagger_408.in1k',\n",
       " 'crossvit_18_240.in1k',\n",
       " 'crossvit_18_dagger_240.in1k',\n",
       " 'crossvit_18_dagger_408.in1k',\n",
       " 'crossvit_base_240.in1k',\n",
       " 'crossvit_small_240.in1k',\n",
       " 'crossvit_tiny_240.in1k',\n",
       " 'cs3darknet_focus_l.c2ns_in1k',\n",
       " 'cs3darknet_focus_m.c2ns_in1k',\n",
       " 'cs3darknet_l.c2ns_in1k',\n",
       " 'cs3darknet_m.c2ns_in1k',\n",
       " 'cs3darknet_x.c2ns_in1k',\n",
       " 'cs3edgenet_x.c2_in1k',\n",
       " 'cs3se_edgenet_x.c2ns_in1k',\n",
       " 'cs3sedarknet_l.c2ns_in1k',\n",
       " 'cs3sedarknet_x.c2ns_in1k',\n",
       " 'cspdarknet53.ra_in1k',\n",
       " 'cspresnet50.ra_in1k',\n",
       " 'cspresnext50.ra_in1k',\n",
       " 'darknet53.c2ns_in1k',\n",
       " 'darknetaa53.c2ns_in1k',\n",
       " 'davit_base.msft_in1k',\n",
       " 'davit_small.msft_in1k',\n",
       " 'davit_tiny.msft_in1k',\n",
       " 'deit3_base_patch16_224.fb_in1k',\n",
       " 'deit3_base_patch16_224.fb_in22k_ft_in1k',\n",
       " 'deit3_base_patch16_384.fb_in1k',\n",
       " 'deit3_base_patch16_384.fb_in22k_ft_in1k',\n",
       " 'deit3_huge_patch14_224.fb_in1k',\n",
       " 'deit3_huge_patch14_224.fb_in22k_ft_in1k',\n",
       " 'deit3_large_patch16_224.fb_in1k',\n",
       " 'deit3_large_patch16_224.fb_in22k_ft_in1k',\n",
       " 'deit3_large_patch16_384.fb_in1k',\n",
       " 'deit3_large_patch16_384.fb_in22k_ft_in1k',\n",
       " 'deit3_medium_patch16_224.fb_in1k',\n",
       " 'deit3_medium_patch16_224.fb_in22k_ft_in1k',\n",
       " 'deit3_small_patch16_224.fb_in1k',\n",
       " 'deit3_small_patch16_224.fb_in22k_ft_in1k',\n",
       " 'deit3_small_patch16_384.fb_in1k',\n",
       " 'deit3_small_patch16_384.fb_in22k_ft_in1k',\n",
       " 'deit_base_distilled_patch16_224.fb_in1k',\n",
       " 'deit_base_distilled_patch16_384.fb_in1k',\n",
       " 'deit_base_patch16_224.fb_in1k',\n",
       " 'deit_base_patch16_384.fb_in1k',\n",
       " 'deit_small_distilled_patch16_224.fb_in1k',\n",
       " 'deit_small_patch16_224.fb_in1k',\n",
       " 'deit_tiny_distilled_patch16_224.fb_in1k',\n",
       " 'deit_tiny_patch16_224.fb_in1k',\n",
       " 'densenet121.ra_in1k',\n",
       " 'densenet121.tv_in1k',\n",
       " 'densenet161.tv_in1k',\n",
       " 'densenet169.tv_in1k',\n",
       " 'densenet201.tv_in1k',\n",
       " 'densenetblur121d.ra_in1k',\n",
       " 'dla34.in1k',\n",
       " 'dla46_c.in1k',\n",
       " 'dla46x_c.in1k',\n",
       " 'dla60.in1k',\n",
       " 'dla60_res2net.in1k',\n",
       " 'dla60_res2next.in1k',\n",
       " 'dla60x.in1k',\n",
       " 'dla60x_c.in1k',\n",
       " 'dla102.in1k',\n",
       " 'dla102x2.in1k',\n",
       " 'dla102x.in1k',\n",
       " 'dla169.in1k',\n",
       " 'dm_nfnet_f0.dm_in1k',\n",
       " 'dm_nfnet_f1.dm_in1k',\n",
       " 'dm_nfnet_f2.dm_in1k',\n",
       " 'dm_nfnet_f3.dm_in1k',\n",
       " 'dm_nfnet_f4.dm_in1k',\n",
       " 'dm_nfnet_f5.dm_in1k',\n",
       " 'dm_nfnet_f6.dm_in1k',\n",
       " 'dpn68.mx_in1k',\n",
       " 'dpn68b.mx_in1k',\n",
       " 'dpn68b.ra_in1k',\n",
       " 'dpn92.mx_in1k',\n",
       " 'dpn98.mx_in1k',\n",
       " 'dpn107.mx_in1k',\n",
       " 'dpn131.mx_in1k',\n",
       " 'eca_botnext26ts_256.c1_in1k',\n",
       " 'eca_halonext26ts.c1_in1k',\n",
       " 'eca_nfnet_l0.ra2_in1k',\n",
       " 'eca_nfnet_l1.ra2_in1k',\n",
       " 'eca_nfnet_l2.ra3_in1k',\n",
       " 'eca_resnet33ts.ra2_in1k',\n",
       " 'eca_resnext26ts.ch_in1k',\n",
       " 'ecaresnet26t.ra2_in1k',\n",
       " 'ecaresnet50d.miil_in1k',\n",
       " 'ecaresnet50d_pruned.miil_in1k',\n",
       " 'ecaresnet50t.a1_in1k',\n",
       " 'ecaresnet50t.a2_in1k',\n",
       " 'ecaresnet50t.a3_in1k',\n",
       " 'ecaresnet50t.ra2_in1k',\n",
       " 'ecaresnet101d.miil_in1k',\n",
       " 'ecaresnet101d_pruned.miil_in1k',\n",
       " 'ecaresnet269d.ra2_in1k',\n",
       " 'ecaresnetlight.miil_in1k',\n",
       " 'edgenext_base.in21k_ft_in1k',\n",
       " 'edgenext_base.usi_in1k',\n",
       " 'edgenext_small.usi_in1k',\n",
       " 'edgenext_small_rw.sw_in1k',\n",
       " 'edgenext_x_small.in1k',\n",
       " 'edgenext_xx_small.in1k',\n",
       " 'efficientformer_l1.snap_dist_in1k',\n",
       " 'efficientformer_l3.snap_dist_in1k',\n",
       " 'efficientformer_l7.snap_dist_in1k',\n",
       " 'efficientformerv2_l.snap_dist_in1k',\n",
       " 'efficientformerv2_s0.snap_dist_in1k',\n",
       " 'efficientformerv2_s1.snap_dist_in1k',\n",
       " 'efficientformerv2_s2.snap_dist_in1k',\n",
       " 'efficientnet_b0.ra_in1k',\n",
       " 'efficientnet_b1.ft_in1k',\n",
       " 'efficientnet_b1_pruned.in1k',\n",
       " 'efficientnet_b2.ra_in1k',\n",
       " 'efficientnet_b2_pruned.in1k',\n",
       " 'efficientnet_b3.ra2_in1k',\n",
       " 'efficientnet_b3_pruned.in1k',\n",
       " 'efficientnet_b4.ra2_in1k',\n",
       " 'efficientnet_b5.sw_in12k',\n",
       " 'efficientnet_b5.sw_in12k_ft_in1k',\n",
       " 'efficientnet_el.ra_in1k',\n",
       " 'efficientnet_el_pruned.in1k',\n",
       " 'efficientnet_em.ra2_in1k',\n",
       " 'efficientnet_es.ra_in1k',\n",
       " 'efficientnet_es_pruned.in1k',\n",
       " 'efficientnet_lite0.ra_in1k',\n",
       " 'efficientnetv2_rw_m.agc_in1k',\n",
       " 'efficientnetv2_rw_s.ra2_in1k',\n",
       " 'efficientnetv2_rw_t.ra2_in1k',\n",
       " 'efficientvit_b0.r224_in1k',\n",
       " 'efficientvit_b1.r224_in1k',\n",
       " 'efficientvit_b1.r256_in1k',\n",
       " 'efficientvit_b1.r288_in1k',\n",
       " 'efficientvit_b2.r224_in1k',\n",
       " 'efficientvit_b2.r256_in1k',\n",
       " 'efficientvit_b2.r288_in1k',\n",
       " 'efficientvit_b3.r224_in1k',\n",
       " 'efficientvit_b3.r256_in1k',\n",
       " 'efficientvit_b3.r288_in1k',\n",
       " 'efficientvit_l1.r224_in1k',\n",
       " 'efficientvit_l2.r224_in1k',\n",
       " 'efficientvit_l2.r256_in1k',\n",
       " 'efficientvit_l2.r288_in1k',\n",
       " 'efficientvit_l2.r384_in1k',\n",
       " 'efficientvit_l3.r224_in1k',\n",
       " 'efficientvit_l3.r256_in1k',\n",
       " 'efficientvit_l3.r320_in1k',\n",
       " 'efficientvit_l3.r384_in1k',\n",
       " 'efficientvit_m0.r224_in1k',\n",
       " 'efficientvit_m1.r224_in1k',\n",
       " 'efficientvit_m2.r224_in1k',\n",
       " 'efficientvit_m3.r224_in1k',\n",
       " 'efficientvit_m4.r224_in1k',\n",
       " 'efficientvit_m5.r224_in1k',\n",
       " 'ese_vovnet19b_dw.ra_in1k',\n",
       " 'ese_vovnet39b.ra_in1k',\n",
       " 'eva02_base_patch14_224.mim_in22k',\n",
       " 'eva02_base_patch14_448.mim_in22k_ft_in1k',\n",
       " 'eva02_base_patch14_448.mim_in22k_ft_in22k',\n",
       " 'eva02_base_patch14_448.mim_in22k_ft_in22k_in1k',\n",
       " 'eva02_base_patch16_clip_224.merged2b',\n",
       " 'eva02_enormous_patch14_clip_224.laion2b',\n",
       " 'eva02_enormous_patch14_clip_224.laion2b_plus',\n",
       " 'eva02_large_patch14_224.mim_in22k',\n",
       " 'eva02_large_patch14_224.mim_m38m',\n",
       " 'eva02_large_patch14_448.mim_in22k_ft_in1k',\n",
       " 'eva02_large_patch14_448.mim_in22k_ft_in22k',\n",
       " 'eva02_large_patch14_448.mim_in22k_ft_in22k_in1k',\n",
       " 'eva02_large_patch14_448.mim_m38m_ft_in1k',\n",
       " 'eva02_large_patch14_448.mim_m38m_ft_in22k',\n",
       " 'eva02_large_patch14_448.mim_m38m_ft_in22k_in1k',\n",
       " 'eva02_large_patch14_clip_224.merged2b',\n",
       " 'eva02_large_patch14_clip_336.merged2b',\n",
       " 'eva02_small_patch14_224.mim_in22k',\n",
       " 'eva02_small_patch14_336.mim_in22k_ft_in1k',\n",
       " 'eva02_tiny_patch14_224.mim_in22k',\n",
       " 'eva02_tiny_patch14_336.mim_in22k_ft_in1k',\n",
       " 'eva_giant_patch14_224.clip_ft_in1k',\n",
       " 'eva_giant_patch14_336.clip_ft_in1k',\n",
       " 'eva_giant_patch14_336.m30m_ft_in22k_in1k',\n",
       " 'eva_giant_patch14_560.m30m_ft_in22k_in1k',\n",
       " 'eva_giant_patch14_clip_224.laion400m',\n",
       " 'eva_giant_patch14_clip_224.merged2b',\n",
       " 'eva_large_patch14_196.in22k_ft_in1k',\n",
       " 'eva_large_patch14_196.in22k_ft_in22k_in1k',\n",
       " 'eva_large_patch14_336.in22k_ft_in1k',\n",
       " 'eva_large_patch14_336.in22k_ft_in22k_in1k',\n",
       " 'fastvit_ma36.apple_dist_in1k',\n",
       " 'fastvit_ma36.apple_in1k',\n",
       " 'fastvit_s12.apple_dist_in1k',\n",
       " 'fastvit_s12.apple_in1k',\n",
       " 'fastvit_sa12.apple_dist_in1k',\n",
       " 'fastvit_sa12.apple_in1k',\n",
       " 'fastvit_sa24.apple_dist_in1k',\n",
       " 'fastvit_sa24.apple_in1k',\n",
       " 'fastvit_sa36.apple_dist_in1k',\n",
       " 'fastvit_sa36.apple_in1k',\n",
       " 'fastvit_t8.apple_dist_in1k',\n",
       " 'fastvit_t8.apple_in1k',\n",
       " 'fastvit_t12.apple_dist_in1k',\n",
       " 'fastvit_t12.apple_in1k',\n",
       " 'fbnetc_100.rmsp_in1k',\n",
       " 'fbnetv3_b.ra2_in1k',\n",
       " 'fbnetv3_d.ra2_in1k',\n",
       " 'fbnetv3_g.ra2_in1k',\n",
       " 'flexivit_base.300ep_in1k',\n",
       " 'flexivit_base.300ep_in21k',\n",
       " 'flexivit_base.600ep_in1k',\n",
       " 'flexivit_base.1000ep_in21k',\n",
       " 'flexivit_base.1200ep_in1k',\n",
       " 'flexivit_base.patch16_in21k',\n",
       " 'flexivit_base.patch30_in21k',\n",
       " 'flexivit_large.300ep_in1k',\n",
       " 'flexivit_large.600ep_in1k',\n",
       " 'flexivit_large.1200ep_in1k',\n",
       " 'flexivit_small.300ep_in1k',\n",
       " 'flexivit_small.600ep_in1k',\n",
       " 'flexivit_small.1200ep_in1k',\n",
       " 'focalnet_base_lrf.ms_in1k',\n",
       " 'focalnet_base_srf.ms_in1k',\n",
       " 'focalnet_huge_fl3.ms_in22k',\n",
       " 'focalnet_huge_fl4.ms_in22k',\n",
       " 'focalnet_large_fl3.ms_in22k',\n",
       " 'focalnet_large_fl4.ms_in22k',\n",
       " 'focalnet_small_lrf.ms_in1k',\n",
       " 'focalnet_small_srf.ms_in1k',\n",
       " 'focalnet_tiny_lrf.ms_in1k',\n",
       " 'focalnet_tiny_srf.ms_in1k',\n",
       " 'focalnet_xlarge_fl3.ms_in22k',\n",
       " 'focalnet_xlarge_fl4.ms_in22k',\n",
       " 'gc_efficientnetv2_rw_t.agc_in1k',\n",
       " 'gcresnet33ts.ra2_in1k',\n",
       " 'gcresnet50t.ra2_in1k',\n",
       " 'gcresnext26ts.ch_in1k',\n",
       " 'gcresnext50ts.ch_in1k',\n",
       " 'gcvit_base.in1k',\n",
       " 'gcvit_small.in1k',\n",
       " 'gcvit_tiny.in1k',\n",
       " 'gcvit_xtiny.in1k',\n",
       " 'gcvit_xxtiny.in1k',\n",
       " 'gernet_l.idstcv_in1k',\n",
       " 'gernet_m.idstcv_in1k',\n",
       " 'gernet_s.idstcv_in1k',\n",
       " 'ghostnet_100.in1k',\n",
       " 'ghostnetv2_100.in1k',\n",
       " 'ghostnetv2_130.in1k',\n",
       " 'ghostnetv2_160.in1k',\n",
       " 'gmixer_24_224.ra3_in1k',\n",
       " 'gmlp_s16_224.ra3_in1k',\n",
       " 'halo2botnet50ts_256.a1h_in1k',\n",
       " 'halonet26t.a1h_in1k',\n",
       " 'halonet50ts.a1h_in1k',\n",
       " 'haloregnetz_b.ra3_in1k',\n",
       " 'hardcorenas_a.miil_green_in1k',\n",
       " 'hardcorenas_b.miil_green_in1k',\n",
       " 'hardcorenas_c.miil_green_in1k',\n",
       " 'hardcorenas_d.miil_green_in1k',\n",
       " 'hardcorenas_e.miil_green_in1k',\n",
       " 'hardcorenas_f.miil_green_in1k',\n",
       " 'hrnet_w18.ms_aug_in1k',\n",
       " 'hrnet_w18.ms_in1k',\n",
       " 'hrnet_w18_small.gluon_in1k',\n",
       " 'hrnet_w18_small.ms_in1k',\n",
       " 'hrnet_w18_small_v2.gluon_in1k',\n",
       " 'hrnet_w18_small_v2.ms_in1k',\n",
       " 'hrnet_w18_ssld.paddle_in1k',\n",
       " 'hrnet_w30.ms_in1k',\n",
       " 'hrnet_w32.ms_in1k',\n",
       " 'hrnet_w40.ms_in1k',\n",
       " 'hrnet_w44.ms_in1k',\n",
       " 'hrnet_w48.ms_in1k',\n",
       " 'hrnet_w48_ssld.paddle_in1k',\n",
       " 'hrnet_w64.ms_in1k',\n",
       " 'inception_next_base.sail_in1k',\n",
       " 'inception_next_base.sail_in1k_384',\n",
       " 'inception_next_small.sail_in1k',\n",
       " 'inception_next_tiny.sail_in1k',\n",
       " 'inception_resnet_v2.tf_ens_adv_in1k',\n",
       " 'inception_resnet_v2.tf_in1k',\n",
       " 'inception_v3.gluon_in1k',\n",
       " 'inception_v3.tf_adv_in1k',\n",
       " 'inception_v3.tf_in1k',\n",
       " 'inception_v3.tv_in1k',\n",
       " 'inception_v4.tf_in1k',\n",
       " 'lambda_resnet26rpt_256.c1_in1k',\n",
       " 'lambda_resnet26t.c1_in1k',\n",
       " 'lambda_resnet50ts.a1h_in1k',\n",
       " 'lamhalobotnet50ts_256.a1h_in1k',\n",
       " 'lcnet_050.ra2_in1k',\n",
       " 'lcnet_075.ra2_in1k',\n",
       " 'lcnet_100.ra2_in1k',\n",
       " 'legacy_senet154.in1k',\n",
       " 'legacy_seresnet18.in1k',\n",
       " 'legacy_seresnet34.in1k',\n",
       " 'legacy_seresnet50.in1k',\n",
       " 'legacy_seresnet101.in1k',\n",
       " 'legacy_seresnet152.in1k',\n",
       " 'legacy_seresnext26_32x4d.in1k',\n",
       " 'legacy_seresnext50_32x4d.in1k',\n",
       " 'legacy_seresnext101_32x4d.in1k',\n",
       " 'legacy_xception.tf_in1k',\n",
       " 'levit_128.fb_dist_in1k',\n",
       " 'levit_128s.fb_dist_in1k',\n",
       " 'levit_192.fb_dist_in1k',\n",
       " 'levit_256.fb_dist_in1k',\n",
       " 'levit_384.fb_dist_in1k',\n",
       " 'levit_conv_128.fb_dist_in1k',\n",
       " 'levit_conv_128s.fb_dist_in1k',\n",
       " 'levit_conv_192.fb_dist_in1k',\n",
       " 'levit_conv_256.fb_dist_in1k',\n",
       " 'levit_conv_384.fb_dist_in1k',\n",
       " 'maxvit_base_tf_224.in1k',\n",
       " 'maxvit_base_tf_224.in21k',\n",
       " 'maxvit_base_tf_384.in1k',\n",
       " 'maxvit_base_tf_384.in21k_ft_in1k',\n",
       " 'maxvit_base_tf_512.in1k',\n",
       " 'maxvit_base_tf_512.in21k_ft_in1k',\n",
       " 'maxvit_large_tf_224.in1k',\n",
       " 'maxvit_large_tf_224.in21k',\n",
       " 'maxvit_large_tf_384.in1k',\n",
       " 'maxvit_large_tf_384.in21k_ft_in1k',\n",
       " 'maxvit_large_tf_512.in1k',\n",
       " 'maxvit_large_tf_512.in21k_ft_in1k',\n",
       " 'maxvit_nano_rw_256.sw_in1k',\n",
       " 'maxvit_rmlp_base_rw_224.sw_in12k',\n",
       " 'maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k',\n",
       " 'maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k',\n",
       " 'maxvit_rmlp_nano_rw_256.sw_in1k',\n",
       " 'maxvit_rmlp_pico_rw_256.sw_in1k',\n",
       " 'maxvit_rmlp_small_rw_224.sw_in1k',\n",
       " 'maxvit_rmlp_tiny_rw_256.sw_in1k',\n",
       " 'maxvit_small_tf_224.in1k',\n",
       " 'maxvit_small_tf_384.in1k',\n",
       " 'maxvit_small_tf_512.in1k',\n",
       " 'maxvit_tiny_rw_224.sw_in1k',\n",
       " 'maxvit_tiny_tf_224.in1k',\n",
       " 'maxvit_tiny_tf_384.in1k',\n",
       " 'maxvit_tiny_tf_512.in1k',\n",
       " 'maxvit_xlarge_tf_224.in21k',\n",
       " 'maxvit_xlarge_tf_384.in21k_ft_in1k',\n",
       " 'maxvit_xlarge_tf_512.in21k_ft_in1k',\n",
       " 'maxxvit_rmlp_nano_rw_256.sw_in1k',\n",
       " 'maxxvit_rmlp_small_rw_256.sw_in1k',\n",
       " 'maxxvitv2_nano_rw_256.sw_in1k',\n",
       " 'maxxvitv2_rmlp_base_rw_224.sw_in12k',\n",
       " 'maxxvitv2_rmlp_base_rw_224.sw_in12k_ft_in1k',\n",
       " 'maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k',\n",
       " 'mixer_b16_224.goog_in21k',\n",
       " 'mixer_b16_224.goog_in21k_ft_in1k',\n",
       " 'mixer_b16_224.miil_in21k',\n",
       " 'mixer_b16_224.miil_in21k_ft_in1k',\n",
       " 'mixer_l16_224.goog_in21k',\n",
       " 'mixer_l16_224.goog_in21k_ft_in1k',\n",
       " 'mixnet_l.ft_in1k',\n",
       " 'mixnet_m.ft_in1k',\n",
       " 'mixnet_s.ft_in1k',\n",
       " 'mixnet_xl.ra_in1k',\n",
       " 'mnasnet_100.rmsp_in1k',\n",
       " 'mnasnet_small.lamb_in1k',\n",
       " 'mobilenetv2_050.lamb_in1k',\n",
       " 'mobilenetv2_100.ra_in1k',\n",
       " 'mobilenetv2_110d.ra_in1k',\n",
       " 'mobilenetv2_120d.ra_in1k',\n",
       " 'mobilenetv2_140.ra_in1k',\n",
       " 'mobilenetv3_large_100.miil_in21k',\n",
       " 'mobilenetv3_large_100.miil_in21k_ft_in1k',\n",
       " 'mobilenetv3_large_100.ra_in1k',\n",
       " 'mobilenetv3_rw.rmsp_in1k',\n",
       " 'mobilenetv3_small_050.lamb_in1k',\n",
       " 'mobilenetv3_small_075.lamb_in1k',\n",
       " 'mobilenetv3_small_100.lamb_in1k',\n",
       " 'mobileone_s0.apple_in1k',\n",
       " 'mobileone_s1.apple_in1k',\n",
       " 'mobileone_s2.apple_in1k',\n",
       " 'mobileone_s3.apple_in1k',\n",
       " 'mobileone_s4.apple_in1k',\n",
       " 'mobilevit_s.cvnets_in1k',\n",
       " 'mobilevit_xs.cvnets_in1k',\n",
       " 'mobilevit_xxs.cvnets_in1k',\n",
       " 'mobilevitv2_050.cvnets_in1k',\n",
       " 'mobilevitv2_075.cvnets_in1k',\n",
       " 'mobilevitv2_100.cvnets_in1k',\n",
       " 'mobilevitv2_125.cvnets_in1k',\n",
       " 'mobilevitv2_150.cvnets_in1k',\n",
       " 'mobilevitv2_150.cvnets_in22k_ft_in1k',\n",
       " 'mobilevitv2_150.cvnets_in22k_ft_in1k_384',\n",
       " 'mobilevitv2_175.cvnets_in1k',\n",
       " 'mobilevitv2_175.cvnets_in22k_ft_in1k',\n",
       " 'mobilevitv2_175.cvnets_in22k_ft_in1k_384',\n",
       " 'mobilevitv2_200.cvnets_in1k',\n",
       " 'mobilevitv2_200.cvnets_in22k_ft_in1k',\n",
       " 'mobilevitv2_200.cvnets_in22k_ft_in1k_384',\n",
       " 'mvitv2_base.fb_in1k',\n",
       " 'mvitv2_base_cls.fb_inw21k',\n",
       " 'mvitv2_huge_cls.fb_inw21k',\n",
       " 'mvitv2_large.fb_in1k',\n",
       " 'mvitv2_large_cls.fb_inw21k',\n",
       " 'mvitv2_small.fb_in1k',\n",
       " 'mvitv2_tiny.fb_in1k',\n",
       " 'nasnetalarge.tf_in1k',\n",
       " 'nest_base_jx.goog_in1k',\n",
       " 'nest_small_jx.goog_in1k',\n",
       " 'nest_tiny_jx.goog_in1k',\n",
       " 'nf_regnet_b1.ra2_in1k',\n",
       " 'nf_resnet50.ra2_in1k',\n",
       " 'nfnet_l0.ra2_in1k',\n",
       " 'pit_b_224.in1k',\n",
       " 'pit_b_distilled_224.in1k',\n",
       " 'pit_s_224.in1k',\n",
       " 'pit_s_distilled_224.in1k',\n",
       " 'pit_ti_224.in1k',\n",
       " 'pit_ti_distilled_224.in1k',\n",
       " 'pit_xs_224.in1k',\n",
       " 'pit_xs_distilled_224.in1k',\n",
       " 'pnasnet5large.tf_in1k',\n",
       " 'poolformer_m36.sail_in1k',\n",
       " 'poolformer_m48.sail_in1k',\n",
       " 'poolformer_s12.sail_in1k',\n",
       " 'poolformer_s24.sail_in1k',\n",
       " 'poolformer_s36.sail_in1k',\n",
       " 'poolformerv2_m36.sail_in1k',\n",
       " 'poolformerv2_m48.sail_in1k',\n",
       " 'poolformerv2_s12.sail_in1k',\n",
       " 'poolformerv2_s24.sail_in1k',\n",
       " 'poolformerv2_s36.sail_in1k',\n",
       " 'pvt_v2_b0.in1k',\n",
       " 'pvt_v2_b1.in1k',\n",
       " 'pvt_v2_b2.in1k',\n",
       " 'pvt_v2_b2_li.in1k',\n",
       " 'pvt_v2_b3.in1k',\n",
       " 'pvt_v2_b4.in1k',\n",
       " 'pvt_v2_b5.in1k',\n",
       " 'regnetv_040.ra3_in1k',\n",
       " 'regnetv_064.ra3_in1k',\n",
       " 'regnetx_002.pycls_in1k',\n",
       " 'regnetx_004.pycls_in1k',\n",
       " 'regnetx_004_tv.tv2_in1k',\n",
       " 'regnetx_006.pycls_in1k',\n",
       " 'regnetx_008.pycls_in1k',\n",
       " 'regnetx_008.tv2_in1k',\n",
       " 'regnetx_016.pycls_in1k',\n",
       " 'regnetx_016.tv2_in1k',\n",
       " 'regnetx_032.pycls_in1k',\n",
       " 'regnetx_032.tv2_in1k',\n",
       " 'regnetx_040.pycls_in1k',\n",
       " 'regnetx_064.pycls_in1k',\n",
       " 'regnetx_080.pycls_in1k',\n",
       " 'regnetx_080.tv2_in1k',\n",
       " 'regnetx_120.pycls_in1k',\n",
       " 'regnetx_160.pycls_in1k',\n",
       " 'regnetx_160.tv2_in1k',\n",
       " 'regnetx_320.pycls_in1k',\n",
       " 'regnetx_320.tv2_in1k',\n",
       " 'regnety_002.pycls_in1k',\n",
       " 'regnety_004.pycls_in1k',\n",
       " 'regnety_004.tv2_in1k',\n",
       " 'regnety_006.pycls_in1k',\n",
       " 'regnety_008.pycls_in1k',\n",
       " 'regnety_008_tv.tv2_in1k',\n",
       " 'regnety_016.pycls_in1k',\n",
       " 'regnety_016.tv2_in1k',\n",
       " 'regnety_032.pycls_in1k',\n",
       " 'regnety_032.ra_in1k',\n",
       " 'regnety_032.tv2_in1k',\n",
       " 'regnety_040.pycls_in1k',\n",
       " 'regnety_040.ra3_in1k',\n",
       " 'regnety_064.pycls_in1k',\n",
       " 'regnety_064.ra3_in1k',\n",
       " 'regnety_080.pycls_in1k',\n",
       " 'regnety_080.ra3_in1k',\n",
       " 'regnety_080_tv.tv2_in1k',\n",
       " 'regnety_120.pycls_in1k',\n",
       " 'regnety_120.sw_in12k',\n",
       " 'regnety_120.sw_in12k_ft_in1k',\n",
       " 'regnety_160.deit_in1k',\n",
       " 'regnety_160.lion_in12k_ft_in1k',\n",
       " 'regnety_160.pycls_in1k',\n",
       " 'regnety_160.sw_in12k',\n",
       " 'regnety_160.sw_in12k_ft_in1k',\n",
       " 'regnety_160.swag_ft_in1k',\n",
       " 'regnety_160.swag_lc_in1k',\n",
       " 'regnety_160.tv2_in1k',\n",
       " 'regnety_320.pycls_in1k',\n",
       " 'regnety_320.seer',\n",
       " 'regnety_320.seer_ft_in1k',\n",
       " 'regnety_320.swag_ft_in1k',\n",
       " 'regnety_320.swag_lc_in1k',\n",
       " 'regnety_320.tv2_in1k',\n",
       " 'regnety_640.seer',\n",
       " 'regnety_640.seer_ft_in1k',\n",
       " 'regnety_1280.seer',\n",
       " 'regnety_1280.seer_ft_in1k',\n",
       " 'regnety_1280.swag_ft_in1k',\n",
       " 'regnety_1280.swag_lc_in1k',\n",
       " 'regnety_2560.seer_ft_in1k',\n",
       " 'regnetz_040.ra3_in1k',\n",
       " 'regnetz_040_h.ra3_in1k',\n",
       " 'regnetz_b16.ra3_in1k',\n",
       " 'regnetz_c16.ra3_in1k',\n",
       " 'regnetz_c16_evos.ch_in1k',\n",
       " 'regnetz_d8.ra3_in1k',\n",
       " 'regnetz_d8_evos.ch_in1k',\n",
       " 'regnetz_d32.ra3_in1k',\n",
       " 'regnetz_e8.ra3_in1k',\n",
       " 'repghostnet_050.in1k',\n",
       " 'repghostnet_058.in1k',\n",
       " 'repghostnet_080.in1k',\n",
       " 'repghostnet_100.in1k',\n",
       " 'repghostnet_111.in1k',\n",
       " 'repghostnet_130.in1k',\n",
       " 'repghostnet_150.in1k',\n",
       " 'repghostnet_200.in1k',\n",
       " 'repvgg_a0.rvgg_in1k',\n",
       " 'repvgg_a1.rvgg_in1k',\n",
       " 'repvgg_a2.rvgg_in1k',\n",
       " 'repvgg_b0.rvgg_in1k',\n",
       " 'repvgg_b1.rvgg_in1k',\n",
       " 'repvgg_b1g4.rvgg_in1k',\n",
       " 'repvgg_b2.rvgg_in1k',\n",
       " 'repvgg_b2g4.rvgg_in1k',\n",
       " 'repvgg_b3.rvgg_in1k',\n",
       " 'repvgg_b3g4.rvgg_in1k',\n",
       " 'repvgg_d2se.rvgg_in1k',\n",
       " 'repvit_m0_9.dist_300e_in1k',\n",
       " 'repvit_m0_9.dist_450e_in1k',\n",
       " 'repvit_m1.dist_in1k',\n",
       " 'repvit_m1_0.dist_300e_in1k',\n",
       " 'repvit_m1_0.dist_450e_in1k',\n",
       " 'repvit_m1_1.dist_300e_in1k',\n",
       " 'repvit_m1_1.dist_450e_in1k',\n",
       " 'repvit_m1_5.dist_300e_in1k',\n",
       " 'repvit_m1_5.dist_450e_in1k',\n",
       " 'repvit_m2.dist_in1k',\n",
       " 'repvit_m2_3.dist_300e_in1k',\n",
       " 'repvit_m2_3.dist_450e_in1k',\n",
       " 'repvit_m3.dist_in1k',\n",
       " 'res2net50_14w_8s.in1k',\n",
       " 'res2net50_26w_4s.in1k',\n",
       " 'res2net50_26w_6s.in1k',\n",
       " 'res2net50_26w_8s.in1k',\n",
       " 'res2net50_48w_2s.in1k',\n",
       " 'res2net50d.in1k',\n",
       " 'res2net101_26w_4s.in1k',\n",
       " 'res2net101d.in1k',\n",
       " 'res2next50.in1k',\n",
       " 'resmlp_12_224.fb_dino',\n",
       " 'resmlp_12_224.fb_distilled_in1k',\n",
       " 'resmlp_12_224.fb_in1k',\n",
       " 'resmlp_24_224.fb_dino',\n",
       " 'resmlp_24_224.fb_distilled_in1k',\n",
       " 'resmlp_24_224.fb_in1k',\n",
       " 'resmlp_36_224.fb_distilled_in1k',\n",
       " 'resmlp_36_224.fb_in1k',\n",
       " 'resmlp_big_24_224.fb_distilled_in1k',\n",
       " 'resmlp_big_24_224.fb_in1k',\n",
       " 'resmlp_big_24_224.fb_in22k_ft_in1k',\n",
       " 'resnest14d.gluon_in1k',\n",
       " 'resnest26d.gluon_in1k',\n",
       " 'resnest50d.in1k',\n",
       " 'resnest50d_1s4x24d.in1k',\n",
       " 'resnest50d_4s2x40d.in1k',\n",
       " 'resnest101e.in1k',\n",
       " 'resnest200e.in1k',\n",
       " 'resnest269e.in1k',\n",
       " 'resnet10t.c3_in1k',\n",
       " 'resnet14t.c3_in1k',\n",
       " 'resnet18.a1_in1k',\n",
       " 'resnet18.a2_in1k',\n",
       " 'resnet18.a3_in1k',\n",
       " 'resnet18.fb_ssl_yfcc100m_ft_in1k',\n",
       " 'resnet18.fb_swsl_ig1b_ft_in1k',\n",
       " 'resnet18.gluon_in1k',\n",
       " 'resnet18.tv_in1k',\n",
       " 'resnet18d.ra2_in1k',\n",
       " 'resnet26.bt_in1k',\n",
       " 'resnet26d.bt_in1k',\n",
       " 'resnet26t.ra2_in1k',\n",
       " 'resnet32ts.ra2_in1k',\n",
       " 'resnet33ts.ra2_in1k',\n",
       " 'resnet34.a1_in1k',\n",
       " 'resnet34.a2_in1k',\n",
       " 'resnet34.a3_in1k',\n",
       " 'resnet34.bt_in1k',\n",
       " 'resnet34.gluon_in1k',\n",
       " 'resnet34.tv_in1k',\n",
       " 'resnet34d.ra2_in1k',\n",
       " 'resnet50.a1_in1k',\n",
       " 'resnet50.a1h_in1k',\n",
       " 'resnet50.a2_in1k',\n",
       " 'resnet50.a3_in1k',\n",
       " 'resnet50.am_in1k',\n",
       " 'resnet50.b1k_in1k',\n",
       " 'resnet50.b2k_in1k',\n",
       " 'resnet50.bt_in1k',\n",
       " 'resnet50.c1_in1k',\n",
       " 'resnet50.c2_in1k',\n",
       " 'resnet50.d_in1k',\n",
       " 'resnet50.fb_ssl_yfcc100m_ft_in1k',\n",
       " 'resnet50.fb_swsl_ig1b_ft_in1k',\n",
       " 'resnet50.gluon_in1k',\n",
       " 'resnet50.ra_in1k',\n",
       " 'resnet50.ram_in1k',\n",
       " 'resnet50.tv2_in1k',\n",
       " 'resnet50.tv_in1k',\n",
       " 'resnet50_gn.a1h_in1k',\n",
       " 'resnet50c.gluon_in1k',\n",
       " 'resnet50d.a1_in1k',\n",
       " 'resnet50d.a2_in1k',\n",
       " 'resnet50d.a3_in1k',\n",
       " 'resnet50d.gluon_in1k',\n",
       " 'resnet50d.ra2_in1k',\n",
       " 'resnet50s.gluon_in1k',\n",
       " 'resnet51q.ra2_in1k',\n",
       " 'resnet61q.ra2_in1k',\n",
       " 'resnet101.a1_in1k',\n",
       " 'resnet101.a1h_in1k',\n",
       " 'resnet101.a2_in1k',\n",
       " 'resnet101.a3_in1k',\n",
       " 'resnet101.gluon_in1k',\n",
       " 'resnet101.tv2_in1k',\n",
       " 'resnet101.tv_in1k',\n",
       " 'resnet101c.gluon_in1k',\n",
       " 'resnet101d.gluon_in1k',\n",
       " 'resnet101d.ra2_in1k',\n",
       " 'resnet101s.gluon_in1k',\n",
       " 'resnet152.a1_in1k',\n",
       " 'resnet152.a1h_in1k',\n",
       " 'resnet152.a2_in1k',\n",
       " 'resnet152.a3_in1k',\n",
       " 'resnet152.gluon_in1k',\n",
       " 'resnet152.tv2_in1k',\n",
       " 'resnet152.tv_in1k',\n",
       " 'resnet152c.gluon_in1k',\n",
       " 'resnet152d.gluon_in1k',\n",
       " 'resnet152d.ra2_in1k',\n",
       " 'resnet152s.gluon_in1k',\n",
       " 'resnet200d.ra2_in1k',\n",
       " 'resnetaa50.a1h_in1k',\n",
       " 'resnetaa50d.d_in12k',\n",
       " 'resnetaa50d.sw_in12k',\n",
       " 'resnetaa50d.sw_in12k_ft_in1k',\n",
       " 'resnetaa101d.sw_in12k',\n",
       " 'resnetaa101d.sw_in12k_ft_in1k',\n",
       " 'resnetblur50.bt_in1k',\n",
       " 'resnetrs50.tf_in1k',\n",
       " 'resnetrs101.tf_in1k',\n",
       " 'resnetrs152.tf_in1k',\n",
       " 'resnetrs200.tf_in1k',\n",
       " 'resnetrs270.tf_in1k',\n",
       " 'resnetrs350.tf_in1k',\n",
       " 'resnetrs420.tf_in1k',\n",
       " 'resnetv2_50.a1h_in1k',\n",
       " 'resnetv2_50d_evos.ah_in1k',\n",
       " 'resnetv2_50d_gn.ah_in1k',\n",
       " 'resnetv2_50x1_bit.goog_distilled_in1k',\n",
       " 'resnetv2_50x1_bit.goog_in21k',\n",
       " 'resnetv2_50x1_bit.goog_in21k_ft_in1k',\n",
       " 'resnetv2_50x3_bit.goog_in21k',\n",
       " 'resnetv2_50x3_bit.goog_in21k_ft_in1k',\n",
       " 'resnetv2_101.a1h_in1k',\n",
       " 'resnetv2_101x1_bit.goog_in21k',\n",
       " 'resnetv2_101x1_bit.goog_in21k_ft_in1k',\n",
       " 'resnetv2_101x3_bit.goog_in21k',\n",
       " 'resnetv2_101x3_bit.goog_in21k_ft_in1k',\n",
       " 'resnetv2_152x2_bit.goog_in21k',\n",
       " 'resnetv2_152x2_bit.goog_in21k_ft_in1k',\n",
       " 'resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k',\n",
       " 'resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k_384',\n",
       " 'resnetv2_152x4_bit.goog_in21k',\n",
       " 'resnetv2_152x4_bit.goog_in21k_ft_in1k',\n",
       " 'resnext26ts.ra2_in1k',\n",
       " 'resnext50_32x4d.a1_in1k',\n",
       " 'resnext50_32x4d.a1h_in1k',\n",
       " 'resnext50_32x4d.a2_in1k',\n",
       " 'resnext50_32x4d.a3_in1k',\n",
       " 'resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k',\n",
       " 'resnext50_32x4d.fb_swsl_ig1b_ft_in1k',\n",
       " 'resnext50_32x4d.gluon_in1k',\n",
       " 'resnext50_32x4d.ra_in1k',\n",
       " 'resnext50_32x4d.tv2_in1k',\n",
       " 'resnext50_32x4d.tv_in1k',\n",
       " 'resnext50d_32x4d.bt_in1k',\n",
       " 'resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k',\n",
       " 'resnext101_32x4d.fb_swsl_ig1b_ft_in1k',\n",
       " 'resnext101_32x4d.gluon_in1k',\n",
       " 'resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k',\n",
       " 'resnext101_32x8d.fb_swsl_ig1b_ft_in1k',\n",
       " 'resnext101_32x8d.fb_wsl_ig1b_ft_in1k',\n",
       " 'resnext101_32x8d.tv2_in1k',\n",
       " 'resnext101_32x8d.tv_in1k',\n",
       " 'resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k',\n",
       " 'resnext101_32x16d.fb_swsl_ig1b_ft_in1k',\n",
       " 'resnext101_32x16d.fb_wsl_ig1b_ft_in1k',\n",
       " 'resnext101_32x32d.fb_wsl_ig1b_ft_in1k',\n",
       " 'resnext101_64x4d.c1_in1k',\n",
       " 'resnext101_64x4d.gluon_in1k',\n",
       " 'resnext101_64x4d.tv_in1k',\n",
       " 'rexnet_100.nav_in1k',\n",
       " 'rexnet_130.nav_in1k',\n",
       " 'rexnet_150.nav_in1k',\n",
       " 'rexnet_200.nav_in1k',\n",
       " 'rexnet_300.nav_in1k',\n",
       " 'rexnetr_200.sw_in12k',\n",
       " 'rexnetr_200.sw_in12k_ft_in1k',\n",
       " 'rexnetr_300.sw_in12k',\n",
       " 'rexnetr_300.sw_in12k_ft_in1k',\n",
       " 'samvit_base_patch16.sa1b',\n",
       " 'samvit_huge_patch16.sa1b',\n",
       " 'samvit_large_patch16.sa1b',\n",
       " 'sebotnet33ts_256.a1h_in1k',\n",
       " 'sehalonet33ts.ra2_in1k',\n",
       " 'selecsls42b.in1k',\n",
       " 'selecsls60.in1k',\n",
       " 'selecsls60b.in1k',\n",
       " 'semnasnet_075.rmsp_in1k',\n",
       " 'semnasnet_100.rmsp_in1k',\n",
       " 'senet154.gluon_in1k',\n",
       " 'sequencer2d_l.in1k',\n",
       " 'sequencer2d_m.in1k',\n",
       " 'sequencer2d_s.in1k',\n",
       " 'seresnet33ts.ra2_in1k',\n",
       " 'seresnet50.a1_in1k',\n",
       " 'seresnet50.a2_in1k',\n",
       " 'seresnet50.a3_in1k',\n",
       " 'seresnet50.ra2_in1k',\n",
       " 'seresnet152d.ra2_in1k',\n",
       " 'seresnext26d_32x4d.bt_in1k',\n",
       " 'seresnext26t_32x4d.bt_in1k',\n",
       " 'seresnext26ts.ch_in1k',\n",
       " 'seresnext50_32x4d.gluon_in1k',\n",
       " 'seresnext50_32x4d.racm_in1k',\n",
       " 'seresnext101_32x4d.gluon_in1k',\n",
       " 'seresnext101_32x8d.ah_in1k',\n",
       " 'seresnext101_64x4d.gluon_in1k',\n",
       " 'seresnext101d_32x8d.ah_in1k',\n",
       " 'seresnextaa101d_32x8d.ah_in1k',\n",
       " 'seresnextaa101d_32x8d.sw_in12k',\n",
       " 'seresnextaa101d_32x8d.sw_in12k_ft_in1k',\n",
       " 'seresnextaa101d_32x8d.sw_in12k_ft_in1k_288',\n",
       " 'seresnextaa201d_32x8d.sw_in12k',\n",
       " 'seresnextaa201d_32x8d.sw_in12k_ft_in1k_384',\n",
       " 'skresnet18.ra_in1k',\n",
       " 'skresnet34.ra_in1k',\n",
       " 'skresnext50_32x4d.ra_in1k',\n",
       " 'spnasnet_100.rmsp_in1k',\n",
       " 'swin_base_patch4_window7_224.ms_in1k',\n",
       " 'swin_base_patch4_window7_224.ms_in22k',\n",
       " 'swin_base_patch4_window7_224.ms_in22k_ft_in1k',\n",
       " 'swin_base_patch4_window12_384.ms_in1k',\n",
       " 'swin_base_patch4_window12_384.ms_in22k',\n",
       " 'swin_base_patch4_window12_384.ms_in22k_ft_in1k',\n",
       " 'swin_large_patch4_window7_224.ms_in22k',\n",
       " 'swin_large_patch4_window7_224.ms_in22k_ft_in1k',\n",
       " 'swin_large_patch4_window12_384.ms_in22k',\n",
       " 'swin_large_patch4_window12_384.ms_in22k_ft_in1k',\n",
       " 'swin_s3_base_224.ms_in1k',\n",
       " 'swin_s3_small_224.ms_in1k',\n",
       " 'swin_s3_tiny_224.ms_in1k',\n",
       " 'swin_small_patch4_window7_224.ms_in1k',\n",
       " 'swin_small_patch4_window7_224.ms_in22k',\n",
       " 'swin_small_patch4_window7_224.ms_in22k_ft_in1k',\n",
       " 'swin_tiny_patch4_window7_224.ms_in1k',\n",
       " 'swin_tiny_patch4_window7_224.ms_in22k',\n",
       " 'swin_tiny_patch4_window7_224.ms_in22k_ft_in1k',\n",
       " 'swinv2_base_window8_256.ms_in1k',\n",
       " 'swinv2_base_window12_192.ms_in22k',\n",
       " 'swinv2_base_window12to16_192to256.ms_in22k_ft_in1k',\n",
       " 'swinv2_base_window12to24_192to384.ms_in22k_ft_in1k',\n",
       " 'swinv2_base_window16_256.ms_in1k',\n",
       " 'swinv2_cr_small_224.sw_in1k',\n",
       " 'swinv2_cr_small_ns_224.sw_in1k',\n",
       " 'swinv2_cr_tiny_ns_224.sw_in1k',\n",
       " 'swinv2_large_window12_192.ms_in22k',\n",
       " 'swinv2_large_window12to16_192to256.ms_in22k_ft_in1k',\n",
       " 'swinv2_large_window12to24_192to384.ms_in22k_ft_in1k',\n",
       " 'swinv2_small_window8_256.ms_in1k',\n",
       " 'swinv2_small_window16_256.ms_in1k',\n",
       " 'swinv2_tiny_window8_256.ms_in1k',\n",
       " 'swinv2_tiny_window16_256.ms_in1k',\n",
       " 'tf_efficientnet_b0.aa_in1k',\n",
       " 'tf_efficientnet_b0.ap_in1k',\n",
       " 'tf_efficientnet_b0.in1k',\n",
       " 'tf_efficientnet_b0.ns_jft_in1k',\n",
       " 'tf_efficientnet_b1.aa_in1k',\n",
       " 'tf_efficientnet_b1.ap_in1k',\n",
       " 'tf_efficientnet_b1.in1k',\n",
       " 'tf_efficientnet_b1.ns_jft_in1k',\n",
       " 'tf_efficientnet_b2.aa_in1k',\n",
       " 'tf_efficientnet_b2.ap_in1k',\n",
       " 'tf_efficientnet_b2.in1k',\n",
       " 'tf_efficientnet_b2.ns_jft_in1k',\n",
       " 'tf_efficientnet_b3.aa_in1k',\n",
       " 'tf_efficientnet_b3.ap_in1k',\n",
       " 'tf_efficientnet_b3.in1k',\n",
       " 'tf_efficientnet_b3.ns_jft_in1k',\n",
       " 'tf_efficientnet_b4.aa_in1k',\n",
       " 'tf_efficientnet_b4.ap_in1k',\n",
       " 'tf_efficientnet_b4.in1k',\n",
       " 'tf_efficientnet_b4.ns_jft_in1k',\n",
       " 'tf_efficientnet_b5.aa_in1k',\n",
       " 'tf_efficientnet_b5.ap_in1k',\n",
       " 'tf_efficientnet_b5.in1k',\n",
       " 'tf_efficientnet_b5.ns_jft_in1k',\n",
       " 'tf_efficientnet_b5.ra_in1k',\n",
       " 'tf_efficientnet_b6.aa_in1k',\n",
       " 'tf_efficientnet_b6.ap_in1k',\n",
       " 'tf_efficientnet_b6.ns_jft_in1k',\n",
       " 'tf_efficientnet_b7.aa_in1k',\n",
       " 'tf_efficientnet_b7.ap_in1k',\n",
       " 'tf_efficientnet_b7.ns_jft_in1k',\n",
       " 'tf_efficientnet_b7.ra_in1k',\n",
       " 'tf_efficientnet_b8.ap_in1k',\n",
       " 'tf_efficientnet_b8.ra_in1k',\n",
       " 'tf_efficientnet_cc_b0_4e.in1k',\n",
       " ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm.list_models(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c336034ccf44d3aa66a5b385a6d1bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/792M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model = timm.create_model('convnextv2_large.fcmae_ft_in22k_in1k', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNeXt(\n",
       "  (stem): Sequential(\n",
       "    (0): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (1): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (stages): Sequential(\n",
       "    (0): ConvNeXtStage(\n",
       "      (downsample): Identity()\n",
       "      (blocks): Sequential(\n",
       "        (0): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): ConvNeXtStage(\n",
       "      (downsample): Sequential(\n",
       "        (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): ConvNeXtStage(\n",
       "      (downsample): Sequential(\n",
       "        (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (3): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (4): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (5): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (6): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (7): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (8): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (9): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (10): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (11): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (12): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (13): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (14): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (15): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (16): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (17): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (18): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (19): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (20): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (21): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (22): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (23): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (24): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (25): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (26): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): ConvNeXtStage(\n",
       "      (downsample): Sequential(\n",
       "        (0): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Conv2d(768, 1536, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(1536, 1536, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1536)\n",
       "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(1536, 1536, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1536)\n",
       "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(1536, 1536, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1536)\n",
       "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): GlobalResponseNormMlp(\n",
       "            (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (grn): GlobalResponseNorm()\n",
       "            (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm_pre): Identity()\n",
       "  (head): NormMlpClassifierHead(\n",
       "    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
       "    (norm): LayerNorm2d((1536,), eps=1e-06, elementwise_affine=True)\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (pre_logits): Identity()\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (fc): Linear(in_features=1536, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                       Output Shape              Param #\n",
       "==============================================================================================================\n",
       "ConvNeXt                                                     [1, 1000]                 --\n",
       "Sequential: 1-1                                            [1, 192, 56, 56]          --\n",
       "    Conv2d: 2-1                                           [1, 192, 56, 56]          9,408\n",
       "    LayerNorm2d: 2-2                                      [1, 192, 56, 56]          384\n",
       "Sequential: 1-2                                            [1, 1536, 7, 7]           --\n",
       "    ConvNeXtStage: 2-3                                    [1, 192, 56, 56]          --\n",
       "        Identity: 3-1                                    [1, 192, 56, 56]          --\n",
       "        Sequential: 3-2                                  [1, 192, 56, 56]          922,176\n",
       "    ConvNeXtStage: 2-4                                    [1, 384, 28, 28]          --\n",
       "        Sequential: 3-3                                  [1, 384, 28, 28]          295,680\n",
       "        Sequential: 3-4                                  [1, 384, 28, 28]          3,613,824\n",
       "    ConvNeXtStage: 2-5                                    [1, 768, 14, 14]          --\n",
       "        Sequential: 3-5                                  [1, 768, 14, 14]          1,181,184\n",
       "        Sequential: 3-6                                  [1, 768, 14, 14]          128,749,824\n",
       "    ConvNeXtStage: 2-6                                    [1, 1536, 7, 7]           --\n",
       "        Sequential: 3-7                                  [1, 1536, 7, 7]           4,721,664\n",
       "        Sequential: 3-8                                  [1, 1536, 7, 7]           56,922,624\n",
       "Identity: 1-3                                              [1, 1536, 7, 7]           --\n",
       "NormMlpClassifierHead: 1-4                                 [1, 1000]                 --\n",
       "    SelectAdaptivePool2d: 2-7                             [1, 1536, 1, 1]           --\n",
       "        AdaptiveAvgPool2d: 3-9                           [1, 1536, 1, 1]           --\n",
       "        Identity: 3-10                                   [1, 1536, 1, 1]           --\n",
       "    LayerNorm2d: 2-8                                      [1, 1536, 1, 1]           3,072\n",
       "    Flatten: 2-9                                          [1, 1536]                 --\n",
       "    Identity: 2-10                                        [1, 1536]                 --\n",
       "    Dropout: 2-11                                         [1, 1536]                 --\n",
       "    Linear: 2-12                                          [1, 1000]                 1,537,000\n",
       "==============================================================================================================\n",
       "Total params: 197,956,840\n",
       "Trainable params: 197,956,840\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 1.26\n",
       "==============================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 638.26\n",
       "Params size (MB): 791.83\n",
       "Estimated Total Size (MB): 1430.69\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(model, input_size=(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m DEVICE \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m sample_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m224\u001b[39m, \u001b[39m224\u001b[39m)\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[0;32m----> 4\u001b[0m flops \u001b[39m=\u001b[39m profile_macs(model, sample_input)\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(flops \u001b[39m/\u001b[39m \u001b[39m1e9\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/std_env/lib/python3.8/site-packages/torchprofile/profile.py:12\u001b[0m, in \u001b[0;36mprofile_macs\u001b[0;34m(model, args, kwargs, reduction)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprofile_macs\u001b[39m(model, args\u001b[39m=\u001b[39m(), kwargs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, reduction\u001b[39m=\u001b[39m\u001b[39msum\u001b[39m):\n\u001b[1;32m     10\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[0;32m---> 12\u001b[0m     graph \u001b[39m=\u001b[39m trace(model, args, kwargs)\n\u001b[1;32m     13\u001b[0m     \u001b[39mfor\u001b[39;00m node \u001b[39min\u001b[39;00m graph\u001b[39m.\u001b[39mnodes:\n\u001b[1;32m     14\u001b[0m         \u001b[39mfor\u001b[39;00m operators, func \u001b[39min\u001b[39;00m handlers:\n",
      "File \u001b[0;32m~/Desktop/std_env/lib/python3.8/site-packages/torchprofile/utils/trace.py:17\u001b[0m, in \u001b[0;36mtrace\u001b[0;34m(model, args, kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39massert\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m'\u001b[39m\u001b[39mKeyword arguments are not supported for now. \u001b[39m\u001b[39m'\u001b[39m \\\n\u001b[1;32m     14\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mPlease use positional arguments instead!\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[39mwith\u001b[39;00m warnings\u001b[39m.\u001b[39mcatch_warnings(record\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m---> 17\u001b[0m     graph, _ \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49m_get_trace_graph(Flatten(model), args, kwargs)\n\u001b[1;32m     19\u001b[0m variables \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[1;32m     20\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m graph\u001b[39m.\u001b[39mnodes():\n",
      "File \u001b[0;32m~/Desktop/std_env/lib/python3.8/site-packages/torch/jit/_trace.py:1285\u001b[0m, in \u001b[0;36m_get_trace_graph\u001b[0;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(args, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m   1284\u001b[0m     args \u001b[39m=\u001b[39m (args,)\n\u001b[0;32m-> 1285\u001b[0m outs \u001b[39m=\u001b[39m ONNXTracedModule(\n\u001b[1;32m   1286\u001b[0m     f, strict, _force_outplace, return_inputs, _return_inputs_states\n\u001b[1;32m   1287\u001b[0m )(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1288\u001b[0m \u001b[39mreturn\u001b[39;00m outs\n",
      "File \u001b[0;32m~/Desktop/std_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/std_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/std_env/lib/python3.8/site-packages/torch/jit/_trace.py:133\u001b[0m, in \u001b[0;36mONNXTracedModule.forward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(out_vars)\n\u001b[0;32m--> 133\u001b[0m graph, out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_create_graph_by_tracing(\n\u001b[1;32m    134\u001b[0m     wrapper,\n\u001b[1;32m    135\u001b[0m     in_vars \u001b[39m+\u001b[39;49m module_state,\n\u001b[1;32m    136\u001b[0m     _create_interpreter_name_lookup_fn(),\n\u001b[1;32m    137\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstrict,\n\u001b[1;32m    138\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_force_outplace,\n\u001b[1;32m    139\u001b[0m )\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs:\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m graph, outs[\u001b[39m0\u001b[39m], ret_inputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/std_env/lib/python3.8/site-packages/torch/jit/_trace.py:124\u001b[0m, in \u001b[0;36mONNXTracedModule.forward.<locals>.wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs_states:\n\u001b[1;32m    123\u001b[0m     inputs_states\u001b[39m.\u001b[39mappend(_unflatten(in_args, in_desc))\n\u001b[0;32m--> 124\u001b[0m outs\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner(\u001b[39m*\u001b[39;49mtrace_inputs))\n\u001b[1;32m    125\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs_states:\n\u001b[1;32m    126\u001b[0m     inputs_states[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m (inputs_states[\u001b[39m0\u001b[39m], trace_inputs)\n",
      "File \u001b[0;32m~/Desktop/std_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/std_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/std_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1509\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/Desktop/std_env/lib/python3.8/site-packages/torchprofile/utils/flatten.py:29\u001b[0m, in \u001b[0;36mFlatten.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 29\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     30\u001b[0m     \u001b[39mreturn\u001b[39;00m flatten(outputs)\n",
      "File \u001b[0;32m~/Desktop/std_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/std_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/std_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1509\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/Desktop/cources/CV703/CV703_Assignment_1/models/convnextv1.py:228\u001b[0m, in \u001b[0;36mConvTransNeXtTiny.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 228\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv_model\u001b[39m.\u001b[39;49mfeatures(x)\n\u001b[1;32m    230\u001b[0m     b, c, h, w \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39msize()\n\u001b[1;32m    231\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(b, c, h \u001b[39m*\u001b[39m w)\u001b[39m.\u001b[39mpermute(\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/std_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/std_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/std_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1509\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/Desktop/std_env/lib/python3.8/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/std_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/std_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/std_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1509\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/Desktop/std_env/lib/python3.8/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/std_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/std_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/std_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1509\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/Desktop/std_env/lib/python3.8/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/Desktop/std_env/lib/python3.8/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    457\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "\n",
    "sample_input = torch.randn(1, 3, 224, 224).to(DEVICE)\n",
    "\n",
    "flops = profile_macs(model, sample_input)\n",
    "\n",
    "print(flops / 1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rikhat.akizhanov/Desktop/std_env/lib/python3.8/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 200])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model('ConvTransNeXtTiny', pretrained=True, num_classes=200, freeze=True).to(DEVICE)\n",
    "\n",
    "sample_input = torch.randn(32, 3, 224, 224).to(DEVICE)\n",
    "\n",
    "model(sample_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "ConvTransNeXtTiny                                  [1, 200]                  --\n",
       "ConvNeXt: 1-3                                    --                        (recursive)\n",
       "    Sequential: 2-1                             [1, 384, 14, 14]          --\n",
       "        Conv2dNormActivation: 3-1              [1, 96, 56, 56]           (4,896)\n",
       "        Sequential: 3-2                        [1, 96, 56, 56]           (237,888)\n",
       "        Sequential: 3-3                        [1, 192, 28, 28]          (74,112)\n",
       "        Sequential: 3-4                        [1, 192, 28, 28]          (918,144)\n",
       "        Sequential: 3-5                        [1, 384, 14, 14]          (295,680)\n",
       "        Sequential: 3-6                        [1, 384, 14, 14]          (10,817,280)\n",
       "TransformerEncoder: 1-2                          [196, 1, 384]             --\n",
       "    ModuleList: 2-2                             --                        --\n",
       "        TransformerEncoderLayer: 3-7           [196, 1, 384]             1,380,736\n",
       "        TransformerEncoderLayer: 3-8           [196, 1, 384]             1,380,736\n",
       "        TransformerEncoderLayer: 3-9           [196, 1, 384]             1,380,736\n",
       "        TransformerEncoderLayer: 3-10          [196, 1, 384]             1,380,736\n",
       "ConvNeXt: 1-3                                    --                        (recursive)\n",
       "    Sequential: 2-3                             [1, 200]                  --\n",
       "        Linear: 3-11                           [1, 200]                  77,000\n",
       "====================================================================================================\n",
       "Total params: 17,947,944\n",
       "Trainable params: 5,599,944\n",
       "Non-trainable params: 12,348,000\n",
       "Total mult-adds (M): 862.89\n",
       "====================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 137.68\n",
       "Params size (MB): 62.31\n",
       "Estimated Total Size (MB): 200.60\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(model, input_size=(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.784007569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rikhat.akizhanov/Desktop/std_env/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::permute\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/rikhat.akizhanov/Desktop/std_env/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::gelu\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/rikhat.akizhanov/Desktop/std_env/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::empty\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/rikhat.akizhanov/Desktop/std_env/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::bernoulli_\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/rikhat.akizhanov/Desktop/std_env/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::unflatten\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/rikhat.akizhanov/Desktop/std_env/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::unsqueeze\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/rikhat.akizhanov/Desktop/std_env/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::scaled_dot_product_attention\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "sample_input = torch.randn(1, 3, 224, 224).to(DEVICE)\n",
    "\n",
    "flops = profile_macs(model, sample_input)\n",
    "\n",
    "print(flops / 1e9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
